{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c095987-0137-4314-8327-3be747780a7c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Human Activity Recognition - AWS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894db4cb-5b82-4573-8cfe-7b16358b696b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Problem Statement\n",
    "Let’s open the notebook “HAR Model training notebook”. The problem statement for this notebook is: Deploying the Human Activity Recognition problem using the Level 1 MLOps architecture, the aim is to enhance the experience of Blackmi's health app by overcoming the problems faced in the level 0 architecture. Utilising the Human Activity Recognition dataset, we will construct a machine-learning model along with the ML pipelines to categorise user activities for the real-time health alerts using AWS sagemaker studio. Here we will also be monitoring the model performance and deploy the model using different deployment techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e57df07-1b14-4dc2-9860-27cdf0ef8c93",
   "metadata": {},
   "source": [
    "### Approach \n",
    "In this notebook we will be building the level 1 architecture of MLOps, and our major focus would be on creating ML pipeline, model monitoring and model deployment. The major take away for this lesson is to learn:\n",
    "\n",
    "1. Feature engineering with the amazon sagemaker processing \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2b4b33d-0906-428a-9af7-f0ff1cd48d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install s3fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f1123b0-e1a4-4d15-b0eb-0ff261b8f05e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "# Importing all the necessary libraries \n",
    "# Importing pandas and numpy for data preprocessing. \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Boto3 is used for launching the EC2 instances and manipulating s3 buckets.\n",
    "import boto3\n",
    "# Sagemaker is imported for building, training and deploying machine learning models.\n",
    "import sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e63645c-aed6-4890-9d73-93d1b5f6e98b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'us-east-1'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialising new sagemaker session as \"sess\".\n",
    "sess = sagemaker.Session()\n",
    "# Check for necessary permission needed for training and deploying models. \n",
    "role = sagemaker.get_execution_role()\n",
    "# To understand where this session is configured to operate.\n",
    "region = boto3.Session().region_name\n",
    "region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6895d1d8-9c61-4e97-a154-280490992c33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Bucket variable is used for storing the location of the bucket\n",
    "bucket = sess.default_bucket()\n",
    "# Assigning the prefix variable \n",
    "prefix = 'mlops-human-activity-recognition'\n",
    "###\n",
    "\n",
    "input_source = f's3://{bucket}/{prefix}/data/data.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f806c9b-deb0-498f-8fba-0b142619cdfe",
   "metadata": {},
   "source": [
    "## Load Data & Save to S3 Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79901750-55f7-42e2-b3df-f2d81e8d330c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_data_path = './data/train.csv'\n",
    "df = pd.read_csv(input_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38a51043-8511-4e58-a0d4-535dba260a9a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2240, 563)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "806539ab-98dd-4a18-9829-c8474478c1be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.to_csv(f's3://{bucket}/{prefix}/data/data.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "33f4f003-a65e-4957-841e-21629ef22bef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sagemaker-us-east-1-825765393939'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27eab099-4c31-462b-aeaf-46d5bcc9c1d6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Feature Engineering with Amazon SageMaker Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c741aa1c-4236-4157-a207-d1b02e80ba17",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting preprocessing.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile preprocessing.py\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import argparse\n",
    "import os\n",
    "from joblib import dump, load\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "def _parse_args():\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    \n",
    "    # Data, model, and output directories\n",
    "    # model_dir is always passed in from SageMaker. By default this is a S3 path under the default bucket.\n",
    "    parser.add_argument('--filepath', type=str, default='/opt/ml/processing/input/')\n",
    "    parser.add_argument('--filename', type=str, default='data.csv')\n",
    "    parser.add_argument('--outputpath', type=str, default='/opt/ml/processing/output/')\n",
    "\n",
    "    return parser.parse_known_args()\n",
    "\n",
    "\n",
    "def get_top_k_features(X, Y, k):\n",
    "        clf = ExtraTreesClassifier(n_estimators=50)\n",
    "        clf = clf.fit(X, Y)\n",
    "        feature_df = pd.DataFrame(\n",
    "            data=(X.columns, clf.feature_importances_)\n",
    "        ).T.sort_values(by=1, ascending=False)\n",
    "        cols = feature_df.head(k)[0].values\n",
    "        return cols\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    # Process arguments\n",
    "    args, _ = _parse_args()\n",
    "    # Load data\n",
    "    path = os.path.join(args.filepath,args.filename)\n",
    "    print(path)\n",
    "    \n",
    "    # Reading the dataset and performing label encoding \n",
    "    df = pd.read_csv(os.path.join(args.filepath,args.filename))\n",
    "    df.dropna(inplace = True)\n",
    "    le = LabelEncoder()\n",
    "    df['Activity'] = le.fit_transform(df['Activity'])\n",
    "\n",
    "    # Assignining the indepeneded and depended variable \n",
    "    X = df.drop(['Activity'], axis =1)\n",
    "    Y = df['Activity']\n",
    "    \n",
    "    # Extracting top 12 important feature and filtering the dataset\n",
    "    k =12\n",
    "    final_cols = get_top_k_features(X, Y, k)\n",
    "    final_cols = np.append(final_cols,np.array(['Activity']))\n",
    "    df = df[final_cols]\n",
    "    \n",
    "    # Train, test, validation split\n",
    "    # Randomly sort the data then split out first 70%, second 20%, and last 10%\n",
    "    train_data, validation_data, test_data = np.split(df.sample(frac=1, random_state=42), [int(0.8 * len(df)), int(0.9 * len(df))])  \n",
    "    \n",
    "    # Storing of train, validation and test datasets \n",
    "    pd.concat([train_data['Activity'], train_data.drop(['Activity'], axis=1)], axis=1).to_csv(os.path.join(args.outputpath, 'train/train.csv'), index=False, header=False)\n",
    "    pd.concat([validation_data['Activity'], validation_data.drop(['Activity'], axis=1)], axis=1).to_csv(os.path.join(args.outputpath, 'validation/validation.csv'), index=False, header=False)\n",
    "    test_data[['Activity']].to_csv(os.path.join(args.outputpath, 'test/test_y.csv'), index=False, header=False)\n",
    "    test_data.drop(['Activity'], axis=1).to_csv(os.path.join(args.outputpath, 'test/test_x.csv'), index=False, header=False)\n",
    "    \n",
    "    ## Save Features columns\n",
    "    dump(final_cols, os.path.join(args.outputpath, 'feature/feature.joblib'))\n",
    "    ## Save Encoder\n",
    "    dump(le, os.path.join(args.outputpath, 'feature/encoder.joblib'))\n",
    "    \n",
    "    print(\"## Processing complete. Exiting.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5522538f-57f2-457a-8936-b11ca432384c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_path = f\"s3://{bucket}/{prefix}/train\"\n",
    "validation_path = f\"s3://{bucket}/{prefix}/validation\"\n",
    "test_path = f\"s3://{bucket}/{prefix}/test\"\n",
    "feature_path = f\"s3://{bucket}/{prefix}/feature\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aa725f3b-c253-4c84-afd5-7a24e3a7fa81",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating processing-job with name sklearn-ml-train-2024-11-20-17-26-03-779\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "................\u001b[34m/opt/ml/processing/input/data.csv\u001b[0m\n",
      "\u001b[34m## Processing complete. Exiting.\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Importing necessary library for data processing \n",
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sklearn_processor = SKLearnProcessor(\n",
    "    framework_version=\"0.23-1\",\n",
    "    role=get_execution_role(),\n",
    "    instance_type=\"ml.t3.medium\",\n",
    "    instance_count=1, \n",
    "    base_job_name='sklearn-ml-train'\n",
    ")\n",
    "\n",
    "sklearn_processor.run(\n",
    "    code='preprocessing.py',\n",
    "    inputs=[\n",
    "        ProcessingInput(\n",
    "            source=input_source, \n",
    "            destination=\"/opt/ml/processing/input\",\n",
    "            s3_input_mode=\"File\",\n",
    "            s3_data_distribution_type=\"ShardedByS3Key\"\n",
    "        )\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(\n",
    "            output_name=\"train_data\", \n",
    "            source=\"/opt/ml/processing/output/train\",\n",
    "            destination=train_path,\n",
    "        ),\n",
    "        ProcessingOutput(output_name=\"validation_data\", source=\"/opt/ml/processing/output/validation\", destination=validation_path),\n",
    "        ProcessingOutput(output_name=\"test_data\", source=\"/opt/ml/processing/output/test\", destination=test_path),\n",
    "        ProcessingOutput(output_name=\"feature_data\", source=\"/opt/ml/processing/output/feature\", destination=feature_path)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f03df72-150c-403f-a8ec-045800786489",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5c264cd4-10d5-4847-b508-296ea7ee98d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s3_input_train = sagemaker.TrainingInput(s3_data=train_path.format(bucket, prefix), \n",
    "                                                    content_type='csv')\n",
    "s3_input_validation = sagemaker.TrainingInput(s3_data=validation_path.format(bucket, prefix),\n",
    "                                                     content_type='csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1449514a-1c38-489c-a363-e205d4adc54d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing sklearn-train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile sklearn-train.py\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from joblib import dump, load\n",
    "import pandas as pd, numpy as np, os, argparse\n",
    "from io import StringIO\n",
    "## predict\n",
    "\n",
    "# inference function - tells SageMaker how to load the model\n",
    "def model_fn(model_dir):\n",
    "    clf = load(os.path.join(model_dir, \"model.joblib\"))\n",
    "    return clf\n",
    "\n",
    "def input_fn(input_data, content_type):\n",
    "    \"\"\"Parse input data payload\n",
    "\n",
    "    We currently only take csv input. Since we need to process both labelled\n",
    "    and unlabelled data we first determine whether the label column is present\n",
    "    by looking at how many columns were provided.\n",
    "    \"\"\"\n",
    "    if content_type == 'text/csv':\n",
    "        # Read the raw input data as CSV.\n",
    "        df = pd.read_csv(StringIO(input_data), \n",
    "                         header=None)\n",
    "\n",
    "        if len(df.columns) == len(feature_columns_names) + 1:\n",
    "            # This is a labelled example, includes the ring label\n",
    "            df.columns = feature_columns_names + [label_column]\n",
    "        elif len(df.columns) == len(feature_columns_names):\n",
    "            # This is an unlabelled example.\n",
    "            df.columns = feature_columns_names\n",
    "\n",
    "        return df\n",
    "    else:\n",
    "        raise ValueError(\"{} not supported by script!\".format(content_type))\n",
    "\n",
    "\n",
    "def output_fn(prediction, accept):\n",
    "    \"\"\"Format prediction output\n",
    "\n",
    "    The default accept/content-type between containers for serial inference is JSON.\n",
    "    We also want to set the ContentType or mimetype as the same value as accept so the next\n",
    "    container can read the response payload correctly.\n",
    "    \"\"\"\n",
    "    if accept == \"application/json\":\n",
    "        instances = []\n",
    "        for row in prediction.tolist():\n",
    "            instances.append({\"features\": row})\n",
    "\n",
    "        json_output = {\"instances\": instances}\n",
    "\n",
    "        return worker.Response(json.dumps(json_output), accept, mimetype=accept)\n",
    "    elif accept == 'text/csv':\n",
    "        return worker.Response(encoders.encode(prediction, accept), accept, mimetype=accept)\n",
    "    else:\n",
    "        raise RuntimeException(\"{} accept type is not supported by this script.\".format(accept))\n",
    "        \n",
    "def predict_fn(input_data, model):\n",
    "    \"\"\"Preprocess input data\n",
    "\n",
    "    We implement this because the default predict_fn uses .predict(), but our model is a preprocessor\n",
    "    so we want to use .transform().\n",
    "\n",
    "    The output is returned in the following order:\n",
    "\n",
    "        rest of features either one hot encoded or standardized\n",
    "    \"\"\"\n",
    "    features = model.transform(input_data)\n",
    "\n",
    "    if label_column in input_data:\n",
    "        # Return the label (as the first column) and the set of features.\n",
    "        return np.insert(features, 0, input_data[label_column], axis=1)\n",
    "    else:\n",
    "        # Return only the set of features\n",
    "        return features        \n",
    "        \n",
    "# Argument parser\n",
    "def _parse_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    # Hyperparameters\n",
    "    parser.add_argument(\"--n-estimators\", type=int, default=10)\n",
    "    parser.add_argument(\"--min-samples-leaf\", type=int, default=3)\n",
    "    # Data, model, and output directories\n",
    "    parser.add_argument(\"--model-dir\", type=str, default=os.environ.get(\"SM_MODEL_DIR\"))\n",
    "    parser.add_argument(\"--train\", type=str, default=os.environ.get(\"SM_CHANNEL_TRAIN\"))\n",
    "    parser.add_argument(\"--test\", type=str, default=os.environ.get(\"SM_CHANNEL_TEST\"))\n",
    "    parser.add_argument(\"--train-file\", type=str, default=\"train.csv\")\n",
    "    parser.add_argument(\"--test-file\", type=str, default=\"test.csv\")\n",
    "    # Parse the arguments\n",
    "    return parser.parse_known_args()\n",
    "\n",
    "# Main Training Loop\n",
    "if __name__==\"__main__\":\n",
    "    # Process arguments\n",
    "    args, _ = _parse_args()\n",
    "    # Load the dataset\n",
    "    train_df = pd.read_csv(os.path.join(args.train, args.train_file))\n",
    "    test_df = pd.read_csv(os.path.join(args.test, args.test_file))\n",
    "    # Separate X and y\n",
    "    X_train, y_train = train_df.drop(train_df.columns[0], axis=1), train_df[train_df.columns[0]]\n",
    "    X_test, y_test = test_df.drop(test_df.columns[0], axis=1), test_df[test_df.columns[0]]\n",
    "    # Define the model and train it\n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators=args.n_estimators, n_jobs=-1\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    # Evaluate the model performances\n",
    "    print(f'Model Accuracy: {accuracy_score(y_test, model.predict(X_test))}')\n",
    "    dump(model, os.path.join(args.model_dir, 'model.joblib'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5b4fd364-fe2d-4a93-9305-3035093bcfee",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: rf-scikit-2024-11-20-17-41-28-939\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-20 17:41:34 Starting - Starting the training job...\n",
      "2024-11-20 17:41:50 Starting - Preparing the instances for training...\n",
      "2024-11-20 17:42:15 Downloading - Downloading input data...\n",
      "2024-11-20 17:42:40 Downloading - Downloading the training image..\n",
      "2024-11-20 17:43:34 Training - Training image download completed. Training in progress.\n",
      "2024-11-20 17:43:34 Uploading - Uploading generated training model\n",
      "2024-11-20 17:43:34 Completed - Training job completed\n",
      "\u001b[34m2024-11-20 17:43:17,533 sagemaker-containers INFO     Imported framework sagemaker_sklearn_container.training\u001b[0m\n",
      "\u001b[34m2024-11-20 17:43:17,537 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2024-11-20 17:43:17,583 sagemaker_sklearn_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2024-11-20 17:43:17,738 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2024-11-20 17:43:17,750 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2024-11-20 17:43:17,763 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2024-11-20 17:43:17,772 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"test\": \"/opt/ml/input/data/test\",\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_sklearn_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"min-samples-leaf\": 3,\n",
      "        \"n-estimators\": 120,\n",
      "        \"test-file\": \"validation.csv\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"test\": {\n",
      "            \"ContentType\": \"csv\",\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"ContentType\": \"csv\",\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"rf-scikit-2024-11-20-17-41-28-939\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-825765393939/rf-scikit-2024-11-20-17-41-28-939/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"sklearn-train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.m5.xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.m5.xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"sklearn-train.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"min-samples-leaf\":3,\"n-estimators\":120,\"test-file\":\"validation.csv\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=sklearn-train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.m5.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"test\":{\"ContentType\":\"csv\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"ContentType\":\"csv\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"test\",\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=sklearn-train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_sklearn_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-825765393939/rf-scikit-2024-11-20-17-41-28-939/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"test\":\"/opt/ml/input/data/test\",\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_sklearn_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"min-samples-leaf\":3,\"n-estimators\":120,\"test-file\":\"validation.csv\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"test\":{\"ContentType\":\"csv\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"ContentType\":\"csv\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"rf-scikit-2024-11-20-17-41-28-939\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-825765393939/rf-scikit-2024-11-20-17-41-28-939/source/sourcedir.tar.gz\",\"module_name\":\"sklearn-train\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.m5.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"sklearn-train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--min-samples-leaf\",\"3\",\"--n-estimators\",\"120\",\"--test-file\",\"validation.csv\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TEST=/opt/ml/input/data/test\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_MIN-SAMPLES-LEAF=3\u001b[0m\n",
      "\u001b[34mSM_HP_N-ESTIMATORS=120\u001b[0m\n",
      "\u001b[34mSM_HP_TEST-FILE=validation.csv\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/miniconda3/bin:/miniconda3/lib/python37.zip:/miniconda3/lib/python3.7:/miniconda3/lib/python3.7/lib-dynload:/miniconda3/lib/python3.7/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/miniconda3/bin/python sklearn-train.py --min-samples-leaf 3 --n-estimators 120 --test-file validation.csv\u001b[0m\n",
      "\u001b[34mModel Accuracy: 0.9461883408071748\u001b[0m\n",
      "\u001b[34m2024-11-20 17:43:19,268 sagemaker-containers INFO     Reporting training SUCCESS\u001b[0m\n",
      "Training seconds: 79\n",
      "Billable seconds: 79\n"
     ]
    }
   ],
   "source": [
    "# We use the Estimator from the SageMaker Python SDK\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.sklearn.estimator import SKLearn\n",
    "\n",
    "FRAMEWORK_VERSION = \"0.23-1\"\n",
    "\n",
    "# Define the Estimator from SageMaker (Script Mode)\n",
    "sklearn_estimator = SKLearn(\n",
    "    entry_point=\"sklearn-train.py\",\n",
    "    role=get_execution_role(),\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    framework_version=FRAMEWORK_VERSION,\n",
    "    base_job_name=\"rf-scikit\",\n",
    "    metric_definitions=[{\"Name\": \"Accuracy\", \"Regex\": \"Accuracy: ([0-9.]+).*$\"}],\n",
    "    hyperparameters={\n",
    "        \"n-estimators\": 120,\n",
    "        \"min-samples-leaf\": 3,\n",
    "        \"test-file\": \"validation.csv\"\n",
    "    },\n",
    ")\n",
    "\n",
    "# Train the model (~5 minutes)\n",
    "sklearn_estimator.fit({\"train\": s3_input_train, \"test\": s3_input_validation})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f32d72-bfa5-48a3-8372-9085235287dc",
   "metadata": {},
   "source": [
    "## Hyper Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "877aa689-0567-4489-8229-92bdac4f0c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use the Hyperparameter Tuner\n",
    "from sagemaker.tuner import IntegerParameter\n",
    "\n",
    "# Define exploration boundaries\n",
    "hyperparameter_ranges = {\n",
    "    \"n-estimators\": IntegerParameter(100, 200),\n",
    "    \"min-samples-leaf\": IntegerParameter(2, 6)\n",
    "}\n",
    "\n",
    "Optimizer = sagemaker.tuner.HyperparameterTuner(\n",
    "    estimator=sklearn_estimator,\n",
    "    hyperparameter_ranges=hyperparameter_ranges,\n",
    "    base_tuning_job_name=\"RF-tuner\",\n",
    "    objective_type=\"Maximize\",\n",
    "    objective_metric_name=\"Accuracy\",\n",
    "    metric_definitions=[\n",
    "        {\"Name\": \"Accuracy\", \"Regex\": \"Accuracy: ([0-9.]+).*$\"}\n",
    "    ],  # extract tracked metric from logs with regexp\n",
    "    max_jobs=10,\n",
    "    max_parallel_jobs=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5ac8dc2e-fd35-49c4-936d-4c0d3fdc4d9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating hyperparameter tuning job with name: RF-tuner-241120-1746\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...........................................................!\n"
     ]
    }
   ],
   "source": [
    "Optimizer.fit({\"train\": s3_input_train, \"test\": s3_input_validation})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6daf9cd2-e16f-44a0-a5f7-1f5c6e0031ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>min-samples-leaf</th>\n",
       "      <th>n-estimators</th>\n",
       "      <th>TrainingJobName</th>\n",
       "      <th>TrainingJobStatus</th>\n",
       "      <th>FinalObjectiveValue</th>\n",
       "      <th>TrainingStartTime</th>\n",
       "      <th>TrainingEndTime</th>\n",
       "      <th>TrainingElapsedTimeSeconds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.0</td>\n",
       "      <td>171.0</td>\n",
       "      <td>RF-tuner-241120-1746-010-779302a8</td>\n",
       "      <td>Completed</td>\n",
       "      <td>0.941704</td>\n",
       "      <td>2024-11-20 17:51:31+00:00</td>\n",
       "      <td>2024-11-20 17:52:00+00:00</td>\n",
       "      <td>29.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.0</td>\n",
       "      <td>131.0</td>\n",
       "      <td>RF-tuner-241120-1746-009-8d26adb8</td>\n",
       "      <td>Completed</td>\n",
       "      <td>0.946188</td>\n",
       "      <td>2024-11-20 17:51:26+00:00</td>\n",
       "      <td>2024-11-20 17:51:55+00:00</td>\n",
       "      <td>29.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>194.0</td>\n",
       "      <td>RF-tuner-241120-1746-008-9803ff51</td>\n",
       "      <td>Completed</td>\n",
       "      <td>0.946188</td>\n",
       "      <td>2024-11-20 17:50:50+00:00</td>\n",
       "      <td>2024-11-20 17:51:19+00:00</td>\n",
       "      <td>29.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>RF-tuner-241120-1746-007-a6cd8a50</td>\n",
       "      <td>Completed</td>\n",
       "      <td>0.950673</td>\n",
       "      <td>2024-11-20 17:50:41+00:00</td>\n",
       "      <td>2024-11-20 17:51:09+00:00</td>\n",
       "      <td>28.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6.0</td>\n",
       "      <td>199.0</td>\n",
       "      <td>RF-tuner-241120-1746-006-80a559a3</td>\n",
       "      <td>Completed</td>\n",
       "      <td>0.946188</td>\n",
       "      <td>2024-11-20 17:50:03+00:00</td>\n",
       "      <td>2024-11-20 17:50:32+00:00</td>\n",
       "      <td>29.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   min-samples-leaf  n-estimators                    TrainingJobName  \\\n",
       "0               3.0         171.0  RF-tuner-241120-1746-010-779302a8   \n",
       "1               3.0         131.0  RF-tuner-241120-1746-009-8d26adb8   \n",
       "2               3.0         194.0  RF-tuner-241120-1746-008-9803ff51   \n",
       "3               3.0         100.0  RF-tuner-241120-1746-007-a6cd8a50   \n",
       "4               6.0         199.0  RF-tuner-241120-1746-006-80a559a3   \n",
       "\n",
       "  TrainingJobStatus  FinalObjectiveValue         TrainingStartTime  \\\n",
       "0         Completed             0.941704 2024-11-20 17:51:31+00:00   \n",
       "1         Completed             0.946188 2024-11-20 17:51:26+00:00   \n",
       "2         Completed             0.946188 2024-11-20 17:50:50+00:00   \n",
       "3         Completed             0.950673 2024-11-20 17:50:41+00:00   \n",
       "4         Completed             0.946188 2024-11-20 17:50:03+00:00   \n",
       "\n",
       "            TrainingEndTime  TrainingElapsedTimeSeconds  \n",
       "0 2024-11-20 17:52:00+00:00                        29.0  \n",
       "1 2024-11-20 17:51:55+00:00                        29.0  \n",
       "2 2024-11-20 17:51:19+00:00                        29.0  \n",
       "3 2024-11-20 17:51:09+00:00                        28.0  \n",
       "4 2024-11-20 17:50:32+00:00                        29.0  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get tuner results in a df\n",
    "results = Optimizer.analytics().dataframe()\n",
    "while results.empty:\n",
    "    time.sleep(1)\n",
    "    results = Optimizer.analytics().dataframe()\n",
    "results.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b2213a4-c7cd-4f30-b369-937e80160b20",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\n",
      "WARNING:sagemaker.interactive_apps.base_interactive_app:NOTEBOOK_METADATA_FILE detected but failed to get valid domain and user from it.\n",
      "INFO:sagemaker:Creating training-job with name: sagemaker-xgboost-2023-09-26-05-47-34-821\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-09-26 05:47:34 Starting - Starting the training job...\n",
      "2023-09-26 05:47:51 Starting - Preparing the instances for training......\n",
      "2023-09-26 05:48:56 Downloading - Downloading input data\n",
      "2023-09-26 05:48:56 Training - Downloading the training image...\n",
      "2023-09-26 05:49:31 Training - Training image download completed. Training in progress....\u001b[34m[2023-09-26 05:49:50.393 ip-10-0-151-179.ap-south-1.compute.internal:7 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2023-09-26 05:49:50.416 ip-10-0-151-179.ap-south-1.compute.internal:7 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2023-09-26:05:49:50:INFO] Imported framework sagemaker_xgboost_container.training\u001b[0m\n",
      "\u001b[34m[2023-09-26:05:49:50:INFO] Failed to parse hyperparameter objective value multi:softmax to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m[2023-09-26:05:49:50:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2023-09-26:05:49:50:INFO] Running XGBoost Sagemaker in algorithm mode\u001b[0m\n",
      "\u001b[34m[2023-09-26:05:49:50:INFO] Determined 0 GPU(s) available on the instance.\u001b[0m\n",
      "\u001b[34m[2023-09-26:05:49:50:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2023-09-26:05:49:50:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2023-09-26:05:49:50:INFO] File path /opt/ml/input/data/train of input files\u001b[0m\n",
      "\u001b[34m[2023-09-26:05:49:50:INFO] Making smlinks from folder /opt/ml/input/data/train to folder /tmp/sagemaker_xgboost_input_data\u001b[0m\n",
      "\u001b[34m[2023-09-26:05:49:50:INFO] creating symlink between Path /opt/ml/input/data/train/train.csv and destination /tmp/sagemaker_xgboost_input_data/train.csv-2776727441156358946\u001b[0m\n",
      "\u001b[34m[2023-09-26:05:49:50:INFO] files path: /tmp/sagemaker_xgboost_input_data\u001b[0m\n",
      "\u001b[34m[2023-09-26:05:49:50:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2023-09-26:05:49:50:INFO] File path /opt/ml/input/data/validation of input files\u001b[0m\n",
      "\u001b[34m[2023-09-26:05:49:50:INFO] Making smlinks from folder /opt/ml/input/data/validation to folder /tmp/sagemaker_xgboost_input_data\u001b[0m\n",
      "\u001b[34m[2023-09-26:05:49:50:INFO] creating symlink between Path /opt/ml/input/data/validation/validation.csv and destination /tmp/sagemaker_xgboost_input_data/validation.csv2134491415977569373\u001b[0m\n",
      "\u001b[34m[2023-09-26:05:49:50:INFO] files path: /tmp/sagemaker_xgboost_input_data\u001b[0m\n",
      "\u001b[34m[2023-09-26:05:49:50:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2023-09-26:05:49:50:INFO] Single node training.\u001b[0m\n",
      "\u001b[34m[2023-09-26:05:49:50:INFO] Train matrix has 80000 rows and 12 columns\u001b[0m\n",
      "\u001b[34m[2023-09-26:05:49:50:INFO] Validation matrix has 10000 rows\u001b[0m\n",
      "\u001b[34m[2023-09-26 05:49:50.939 ip-10-0-151-179.ap-south-1.compute.internal:7 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2023-09-26 05:49:50.940 ip-10-0-151-179.ap-south-1.compute.internal:7 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2023-09-26 05:49:50.941 ip-10-0-151-179.ap-south-1.compute.internal:7 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2023-09-26 05:49:50.941 ip-10-0-151-179.ap-south-1.compute.internal:7 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2023-09-26:05:49:50:INFO] Debug hook created from config\u001b[0m\n",
      "\u001b[34m[0]#011train-mlogloss:1.42600#011validation-mlogloss:1.42603\u001b[0m\n",
      "\u001b[34m[2023-09-26 05:49:51.283 ip-10-0-151-179.ap-south-1.compute.internal:7 INFO hook.py:427] Monitoring the collections: metrics\u001b[0m\n",
      "\u001b[34m[2023-09-26 05:49:51.285 ip-10-0-151-179.ap-south-1.compute.internal:7 INFO hook.py:491] Hook is writing from the hook with pid: 7\u001b[0m\n",
      "\u001b[34m[1]#011train-mlogloss:1.20841#011validation-mlogloss:1.20899\u001b[0m\n",
      "\u001b[34m[2]#011train-mlogloss:1.05531#011validation-mlogloss:1.05639\u001b[0m\n",
      "\u001b[34m[3]#011train-mlogloss:0.93999#011validation-mlogloss:0.94156\u001b[0m\n",
      "\u001b[34m[4]#011train-mlogloss:0.85060#011validation-mlogloss:0.85237\u001b[0m\n",
      "\u001b[34m[5]#011train-mlogloss:0.77795#011validation-mlogloss:0.78003\u001b[0m\n",
      "\u001b[34m[6]#011train-mlogloss:0.72004#011validation-mlogloss:0.72202\u001b[0m\n",
      "\u001b[34m[7]#011train-mlogloss:0.67251#011validation-mlogloss:0.67463\u001b[0m\n",
      "\u001b[34m[8]#011train-mlogloss:0.63398#011validation-mlogloss:0.63630\u001b[0m\n",
      "\u001b[34m[9]#011train-mlogloss:0.60156#011validation-mlogloss:0.60404\u001b[0m\n",
      "\u001b[34m[10]#011train-mlogloss:0.57472#011validation-mlogloss:0.57712\u001b[0m\n",
      "\u001b[34m[11]#011train-mlogloss:0.55184#011validation-mlogloss:0.55446\u001b[0m\n",
      "\u001b[34m[12]#011train-mlogloss:0.53273#011validation-mlogloss:0.53579\u001b[0m\n",
      "\u001b[34m[13]#011train-mlogloss:0.51628#011validation-mlogloss:0.51936\u001b[0m\n",
      "\u001b[34m[14]#011train-mlogloss:0.50273#011validation-mlogloss:0.50606\u001b[0m\n",
      "\u001b[34m[15]#011train-mlogloss:0.49084#011validation-mlogloss:0.49458\u001b[0m\n",
      "\u001b[34m[16]#011train-mlogloss:0.48069#011validation-mlogloss:0.48450\u001b[0m\n",
      "\u001b[34m[17]#011train-mlogloss:0.47197#011validation-mlogloss:0.47601\u001b[0m\n",
      "\u001b[34m[18]#011train-mlogloss:0.46406#011validation-mlogloss:0.46821\u001b[0m\n",
      "\u001b[34m[19]#011train-mlogloss:0.45724#011validation-mlogloss:0.46170\u001b[0m\n",
      "\u001b[34m[20]#011train-mlogloss:0.45065#011validation-mlogloss:0.45535\u001b[0m\n",
      "\u001b[34m[21]#011train-mlogloss:0.44522#011validation-mlogloss:0.45013\u001b[0m\n",
      "\u001b[34m[22]#011train-mlogloss:0.44062#011validation-mlogloss:0.44572\u001b[0m\n",
      "\u001b[34m[23]#011train-mlogloss:0.43665#011validation-mlogloss:0.44191\u001b[0m\n",
      "\u001b[34m[24]#011train-mlogloss:0.43303#011validation-mlogloss:0.43832\u001b[0m\n",
      "\u001b[34m[25]#011train-mlogloss:0.42955#011validation-mlogloss:0.43524\u001b[0m\n",
      "\u001b[34m[26]#011train-mlogloss:0.42656#011validation-mlogloss:0.43241\u001b[0m\n",
      "\u001b[34m[27]#011train-mlogloss:0.42355#011validation-mlogloss:0.42956\u001b[0m\n",
      "\u001b[34m[28]#011train-mlogloss:0.42119#011validation-mlogloss:0.42731\u001b[0m\n",
      "\u001b[34m[29]#011train-mlogloss:0.41845#011validation-mlogloss:0.42475\u001b[0m\n",
      "\u001b[34m[30]#011train-mlogloss:0.41653#011validation-mlogloss:0.42297\u001b[0m\n",
      "\u001b[34m[31]#011train-mlogloss:0.41464#011validation-mlogloss:0.42121\u001b[0m\n",
      "\u001b[34m[32]#011train-mlogloss:0.41288#011validation-mlogloss:0.41960\u001b[0m\n",
      "\u001b[34m[33]#011train-mlogloss:0.41136#011validation-mlogloss:0.41831\u001b[0m\n",
      "\u001b[34m[34]#011train-mlogloss:0.40975#011validation-mlogloss:0.41687\u001b[0m\n",
      "\u001b[34m[35]#011train-mlogloss:0.40808#011validation-mlogloss:0.41527\u001b[0m\n",
      "\u001b[34m[36]#011train-mlogloss:0.40689#011validation-mlogloss:0.41421\u001b[0m\n",
      "\u001b[34m[37]#011train-mlogloss:0.40585#011validation-mlogloss:0.41330\u001b[0m\n",
      "\u001b[34m[38]#011train-mlogloss:0.40484#011validation-mlogloss:0.41239\u001b[0m\n",
      "\u001b[34m[39]#011train-mlogloss:0.40334#011validation-mlogloss:0.41113\u001b[0m\n",
      "\u001b[34m[40]#011train-mlogloss:0.40231#011validation-mlogloss:0.41027\u001b[0m\n",
      "\u001b[34m[41]#011train-mlogloss:0.40125#011validation-mlogloss:0.40946\u001b[0m\n",
      "\u001b[34m[42]#011train-mlogloss:0.40036#011validation-mlogloss:0.40860\u001b[0m\n",
      "\u001b[34m[43]#011train-mlogloss:0.39943#011validation-mlogloss:0.40785\u001b[0m\n",
      "\u001b[34m[44]#011train-mlogloss:0.39875#011validation-mlogloss:0.40731\u001b[0m\n",
      "\u001b[34m[45]#011train-mlogloss:0.39800#011validation-mlogloss:0.40659\u001b[0m\n",
      "\u001b[34m[46]#011train-mlogloss:0.39733#011validation-mlogloss:0.40626\u001b[0m\n",
      "\u001b[34m[47]#011train-mlogloss:0.39675#011validation-mlogloss:0.40579\u001b[0m\n",
      "\u001b[34m[48]#011train-mlogloss:0.39595#011validation-mlogloss:0.40517\u001b[0m\n",
      "\u001b[34m[49]#011train-mlogloss:0.39511#011validation-mlogloss:0.40467\u001b[0m\n",
      "\n",
      "2023-09-26 05:50:23 Uploading - Uploading generated training model\n",
      "2023-09-26 05:50:23 Completed - Training job completed\n",
      "Training seconds: 102\n",
      "Billable seconds: 102\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "from sagemaker import image_uris\n",
    "from sagemaker.session import Session\n",
    "from sagemaker.inputs import TrainingInput\n",
    "\n",
    "# initialize hyperparameters\n",
    "hyperparameters = {\n",
    "        \"num_class\":6,\n",
    "        \"max_depth\":\"5\",\n",
    "        \"eta\":\"0.2\",\n",
    "        \"gamma\":\"4\",\n",
    "        \"min_child_weight\":\"6\",\n",
    "        \"subsample\":\"0.7\",\n",
    "        \"objective\":\"multi:softmax\",\n",
    "        \"num_round\":\"50\"}\n",
    "\n",
    "# set an output path where the trained model will be saved\n",
    "#bucket = sagemaker.Session().default_bucket()\n",
    "#prefix = 'DEMO-xgboost-as-a-built-in-algo'\n",
    "output_path = 's3://{}/{}/{}/output'.format(bucket, prefix, 'abalone-xgb-built-in-algo')\n",
    "\n",
    "# this line automatically looks for the XGBoost image URI and builds an XGBoost container.\n",
    "# specify the repo_version depending on your preference.\n",
    "xgboost_container = sagemaker.image_uris.retrieve(\"xgboost\", region, \"1.7-1\")\n",
    "\n",
    "# construct a SageMaker estimator that calls the xgboost-container\n",
    "xgboost_estimator = sagemaker.estimator.Estimator(image_uri=xgboost_container, \n",
    "                                          hyperparameters=hyperparameters,\n",
    "                                          role=sagemaker.get_execution_role(),\n",
    "                                          instance_count=1, \n",
    "                                          instance_type='ml.m5.xlarge', \n",
    "                                          volume_size=5, # 5 GB \n",
    "                                          output_path=output_path)\n",
    "\n",
    "# define the data type and paths to the training and validation datasets\n",
    "content_type = \"text/csv\"#\"libsvm\"\n",
    "train_input = TrainingInput(\"s3://{}/{}/{}/\".format(bucket, prefix, 'train'), content_type=content_type)\n",
    "validation_input = TrainingInput(\"s3://{}/{}/{}/\".format(bucket, prefix, 'validation'), content_type=content_type)\n",
    "\n",
    "# execute the XGBoost training job\n",
    "xgboost_estimator.fit({'train': train_input, 'validation': validation_input})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ebe91d-3701-453b-8fa8-7724533e1f12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22959e37-e138-4ecf-b560-45f8d2c20a0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4acf32-3277-4221-878e-b52a2f796d9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c92fbe-ed34-47f1-b30a-cc2c765ec96d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
